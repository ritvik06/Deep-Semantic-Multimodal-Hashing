{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import csv\n",
    "import imageio\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import scipy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import shutil\n",
    "from datetime import date\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create alexnet features\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fe067f47f40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "alexnet.classifier[5].register_forward_hook(get_activation('img_feats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = {1:'art', 2:'biology', 3:'geography', 4:'history', 5:'literature', 6:'media',\\\n",
    "            7:'music', 8:'royalty', 9:'sport', 10:'warfare'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features = sio.loadmat('./wikipedia_dataset/raw_features.mat')\n",
    "\n",
    "img_features, text_features = np.float32(raw_features['I_tr']), np.float32(raw_features['T_tr'])\n",
    "\n",
    "concepts = []\n",
    "#stores tuple (text name, img name, concept)\n",
    "text_img_names = []\n",
    "\n",
    "infile = open('./wikipedia_info/trainset_txt_img_cat.list', 'r')\n",
    "data = infile.readlines()\n",
    "for line in data:\n",
    "    line_arr = line.strip().split('\\t')\n",
    "    text_img_names.append((line_arr[0], line_arr[1], subjects[int(line_arr[2])]))\n",
    "    concepts.append(int(line_arr[2])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/envs/ML_Tools/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n"
     ]
    }
   ],
   "source": [
    "#Make all text and img pairs\n",
    "#Store img feats,text name,text features,img label,text label,label_similarity\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for i in range(len(img_features)):\n",
    "    arr = np.arange(len(img_features))\n",
    "    np.random.shuffle(arr)\n",
    "    \n",
    "    img = Image.open('./wikipedia_dataset/images/' + text_img_names[i][2] + '/' + text_img_names[i][1]+'.jpg')\n",
    "    img = img.convert('RGB')\n",
    "    img = transform(img)\n",
    "    img = img.reshape(1,3,224,224)\n",
    "\n",
    "    output = alexnet(img)\n",
    "    img_feats = activation['img_feats'][0].numpy()\n",
    "    \n",
    "    for j in arr[:100]:\n",
    "        pairs.append((text_img_names[i][2] + '/' + text_img_names[i][1]+'.jpg',\\\n",
    "                      text_img_names[j][2] + '/' + text_img_names[j][0], img_feats,\\\n",
    "                      text_features[j], concepts[i],concepts[j], np.float32(concepts[i]==concepts[j])))\n",
    "    \n",
    "    \n",
    "    pairs.append((text_img_names[i][2] + '/' + text_img_names[i][1]+'.jpg',\\\n",
    "                  text_img_names[i][2] + '/' + text_img_names[i][0], img_feats,\\\n",
    "                          text_features[i], concepts[i],concepts[i], 1.))\n",
    "    \n",
    "    if(not i%100):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('media/ceb47321a83dd824cec2d5d3f2034765.jpg', 'literature/7dde4239baf7365dbedb1a0503f9d322-2', array([0.       , 0.       , 0.718269 , ..., 2.4364696, 0.       ,\n",
      "       0.       ], dtype=float32), array([0.07198273, 0.3217547 , 0.03789706, 0.03646101, 0.03835768,\n",
      "       0.08873656, 0.03675003, 0.17545001, 0.07450258, 0.11810768],\n",
      "      dtype=float32), 5, 4, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \"\"\"\n",
    "    DataLoader class for image features of the WIKI dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_text_pairs, train=True):\n",
    "        #If train is false, we are testing\n",
    "        self.all_pairs = img_text_pairs\n",
    "        \n",
    "        self.num_img_features = len(img_text_pairs[0][0])\n",
    "        self.num_text_features = len(img_text_pairs[0][2])\n",
    "         \n",
    "    def __len__(self):\n",
    "        return len(self.all_pairs)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair_info = self.all_pairs[idx]\n",
    "        return [{'img':pair_info[0], 'text': pair_info[1], 'img_feats': pair_info[2],\n",
    "                 'text_feats': pair_info[3], 'img_label': pair_info[4], 'text_label':pair_info[5],\n",
    "                 'similarity': pair_info[6]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, hashing_length=32, num_of_concepts=10):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.img_hashing_layer = nn.Linear(4096, hashing_length)\n",
    "        nn.init.xavier_normal_(self.img_hashing_layer.weight)\n",
    "        \n",
    "        self.text_hashing_layer = nn.Linear(10, hashing_length)\n",
    "        nn.init.xavier_normal_(self.text_hashing_layer.weight)\n",
    "        \n",
    "        self.img_output = nn.Linear(hashing_length, num_of_concepts)\n",
    "        nn.init.xavier_normal_(self.img_output.weight)\n",
    "        \n",
    "        self.text_output = nn.Linear(hashing_length, num_of_concepts)\n",
    "        nn.init.xavier_normal_(self.text_output.weight)\n",
    "        \n",
    "        self.num_of_concepts = num_of_concepts\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def hash_vector(self, features, img=True):\n",
    "        if img:\n",
    "            hash_code = self.img_hashing_layer(features)\n",
    "        else:\n",
    "            hash_code = self.text_hashing_layer(features)\n",
    "            \n",
    "        hash_code = self.tanh(hash_code)\n",
    "        return hash_code\n",
    "        \n",
    "    def forward(self, img_features, text_features):\n",
    "        img_hash_code = self.img_hashing_layer(img_features)\n",
    "        img_hash_code = self.tanh(img_hash_code)\n",
    "        concept_vector = self.img_output(img_hash_code)\n",
    "        img_output = self.softmax(concept_vector)\n",
    "\n",
    "        text_hash_code = self.text_hashing_layer(text_features)\n",
    "        text_hash_code = self.tanh(text_hash_code)\n",
    "        concept_vector = self.text_output(text_hash_code)\n",
    "        text_output = self.softmax(concept_vector)\n",
    "        \n",
    "        return img_output, text_output, img_hash_code, text_hash_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Correction(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Loss_Correction, self).__init__()\n",
    "\n",
    "    def forward(self, img_hash_batch, text_hash_batch):\n",
    "        #Quantization\n",
    "        loss = torch.norm(torch.abs(img_hash_batch) - torch.ones(img_hash_batch.shape), p='fro')**2\n",
    "        loss += torch.norm(torch.abs(text_hash_batch) - torch.ones(text_hash_batch.shape), p='fro')**2\n",
    "\n",
    "        #Bit Balancing\n",
    "        loss += torch.norm(torch.matmul(torch.ones((1,img_hash_batch.shape[0])), img_hash_batch), p=2)**2\n",
    "        loss += torch.norm(torch.matmul(torch.ones((1,img_hash_batch.shape[0])), text_hash_batch), p=2)**2\n",
    "\n",
    "        return loss/2*(img_hash_batch.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_binary(hash_code):\n",
    "    binary_code = np.ones(len(hash_code), dtype=np.int8)\n",
    "    for i, el in enumerate(hash_code):\n",
    "        if (el<0):\n",
    "            binary_code[i] = -1\n",
    "    \n",
    "    return binary_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_1(img_hash_code, text_hash_code, similarity):    \n",
    "    #Inter Modality Similarity preserving loss\n",
    "    loss1 = nn.MSELoss()\n",
    "    \n",
    "    return loss1(torch.dot(img_hash_code, text_hash_code)/len(img_hash_code), similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_2(img_output, text_output, img_label, text_label): \n",
    "    #Label Preserving loss\n",
    "    loss2 = nn.CrossEntropyLoss()\n",
    "        \n",
    "    return (loss2(img_output, img_label) + loss2(text_output, text_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(img_hash_vec, text_hash_vec):\n",
    "    hm_dist = 0\n",
    "\n",
    "    for i in range(len(img_hash_vec)):\n",
    "        if (img_hash_vec[i]!=text_hash_vec[i]):\n",
    "            hm_dist+=1\n",
    "    \n",
    "    return hm_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_OF_EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataLoader(pairs)\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "train_ds, valid_ds = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds,batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_ds,batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average loss per batch : 10.542\n",
      "Epoch 1, Average loss per batch : 9.872\n",
      "Epoch 2, Average loss per batch : 9.461\n",
      "Epoch 3, Average loss per batch : 9.173\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-90cace37ee2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_hash_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_hash_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_similarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mloss1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_hash_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_hash_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_similarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_img_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_text_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-ef3d5b167cb8>\u001b[0m in \u001b[0;36mloss_fn_1\u001b[0;34m(img_hash_code, text_hash_code, similarity)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_fn_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_hash_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_hash_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#Inter Modality Similarity preserving loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_hash_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_hash_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_hash_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_Tools/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_Tools/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_Tools/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mInitializes\u001b[0m \u001b[0minternal\u001b[0m \u001b[0mModule\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared\u001b[0m \u001b[0mby\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"python.nn_module\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAINING LOOP\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Model(hashing_length=32, num_of_concepts=10)\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adagrad(\n",
    "    [\n",
    "        {\"params\": model.img_hashing_layer.parameters(), \"lr\": 0.01},\n",
    "        {\"params\": model.text_hashing_layer.parameters(), \"lr\": 0.01},\n",
    "        {\"params\": model.img_output.parameters(), \"lr\": 0.001},\n",
    "        {\"params\": model.text_output.parameters(), \"lr\": 0.001},\n",
    "    ],\n",
    "    lr=0.00001,\n",
    "    )\n",
    "\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "alpha, beta = 1, 0.5\n",
    "\n",
    "for epoch in range(NUM_OF_EPOCHS):\n",
    "    total_epoch_loss = 0\n",
    "    for i, data_row in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        text_name_batch, batch_img_feats, batch_text_feats, batch_img_label, batch_text_label, batch_similarity =\\\n",
    "        data_row[0]['text'], data_row[0]['img_feats'], data_row[0]['text_feats'],\\\n",
    "        data_row[0]['img_label'], data_row[0]['text_label'], data_row[0]['similarity']\n",
    "        \n",
    "        img_output, text_output, img_hash_batch, text_hash_batch = model(batch_img_feats, batch_text_feats) \n",
    "        \n",
    "        for j in range(img_output.shape[0]):\n",
    "            if (j==0):\n",
    "                loss1 = loss_fn_1(img_hash_batch[j], text_hash_batch[j], batch_similarity[j])\n",
    "            else:\n",
    "                loss1 += loss_fn_1(img_hash_batch[j], text_hash_batch[j], batch_similarity[j]) \n",
    "\n",
    "        loss2 = loss_fn_2(img_output, text_output, batch_img_label, batch_text_label)\n",
    "\n",
    "        correction = Loss_Correction()\n",
    "        loss3 = correction(img_hash_batch, text_hash_batch)\n",
    "\n",
    "        loss = alpha*(loss1+loss2)\n",
    "#         loss = alpha*(loss1+loss2) + beta*loss3\n",
    "\n",
    "        try:\n",
    "            loss.backward()\n",
    "        except:\n",
    "            ''\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch %d, Average loss per batch : %0.3f\"%(epoch, total_epoch_loss/len(train_loader)))\n",
    "    \n",
    "    if(epoch%10==0):\n",
    "        torch.save(model.state_dict(), \"./saved_models/epoch\" + str(epoch) + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD TRAINED MODEL\n",
    "\n",
    "model = Model(hashing_length=32, num_of_concepts=10)\n",
    "model = model.to('cpu')\n",
    "\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load('./saved_models/epoch230.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss per batch : 7.977\n"
     ]
    }
   ],
   "source": [
    "#TESTING LOOP\n",
    "\n",
    "img_hash = {}\n",
    "text_hash = {}\n",
    "\n",
    "alpha, beta = 1, 0.5\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "for i, data_row in enumerate(valid_loader):\n",
    "    img_name_batch, text_name_batch, batch_img_feats, batch_text_feats, batch_img_label, batch_text_label, batch_similarity =\\\n",
    "    data_row[0]['img'], data_row[0]['text'], data_row[0]['img_feats'], data_row[0]['text_feats'],\\\n",
    "    data_row[0]['img_label'], data_row[0]['text_label'], data_row[0]['similarity']\n",
    "\n",
    "    img_output, text_output, img_hash_batch, text_hash_batch = model(batch_img_feats, batch_text_feats) \n",
    "\n",
    "    for i in range(len(img_name_batch)):\n",
    "        img_hash[img_name_batch[i]] = transform_to_binary(img_hash_batch[i].detach().numpy())\n",
    "        text_hash[text_name_batch[i]] = transform_to_binary(text_hash_batch[i].detach().numpy())\n",
    "    \n",
    "    for j in range(img_output.shape[0]):\n",
    "        if (j==0):\n",
    "            loss1 = loss_fn_1(img_hash_batch[j], text_hash_batch[j], batch_similarity[j])\n",
    "        else:\n",
    "            loss1 += loss_fn_1(img_hash_batch[j], text_hash_batch[j], batch_similarity[j]) \n",
    "\n",
    "    loss2 = loss_fn_2(img_output, text_output, batch_img_label, batch_text_label)\n",
    "\n",
    "    correction = Loss_Correction()\n",
    "    loss3 = correction(img_hash_batch, text_hash_batch)\n",
    "\n",
    "    loss = alpha*(loss1+loss2)\n",
    "#         print(loss.item())\n",
    "#         loss = alpha*(loss1+loss2) + beta*loss3\n",
    "\n",
    "    total_loss+=loss.item()\n",
    "\n",
    "print(\"Average validation loss per batch : %0.3f\"% (total_loss/len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "Average validation loss per batch : 7.977\n"
     ]
    }
   ],
   "source": [
    "for i, data_row in enumerate(valid_loader):\n",
    "    img_name_batch, text_name_batch, batch_img_feats, batch_text_feats, batch_img_label, batch_text_label, batch_similarity =\\\n",
    "    data_row[0]['img'], data_row[0]['text'], data_row[0]['img_feats'], data_row[0]['text_feats'],\\\n",
    "    data_row[0]['img_label'], data_row[0]['text_label'], data_row[0]['similarity']\n",
    "\n",
    "    img_output, text_output, img_hash_batch, text_hash_batch = model(batch_img_feats, batch_text_feats) \n",
    "\n",
    "    print(len(np.where(np.abs(img_hash_batch.detach().numpy())<0.99)[0]))\n",
    "    break\n",
    "\n",
    "print(\"Average validation loss per batch : %0.3f\"% (total_loss/len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRECISION@K\n",
    "K = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@100 for Image-Query-Text is 0.55\n",
      "Average Precision@100 for Image-Query-Image is 0.58\n"
     ]
    }
   ],
   "source": [
    "#Image-Query-Text\n",
    "img_cnt, text_cnt = 0., 0.\n",
    "\n",
    "for query_img in img_hash.keys():\n",
    "    \n",
    "    img_hm_dist = {}\n",
    "    text_hm_dist = {}\n",
    "\n",
    "    for text in text_hash.keys():\n",
    "        text_hm_dist[text] = hamming_distance(img_hash[query_img], text_hash[text])\n",
    "\n",
    "    #Closest-5 texts\n",
    "    text_response = sorted(text_hm_dist.items(), key=lambda item: item[1])\n",
    "\n",
    "    for img in img_hash.keys():\n",
    "        img_hm_dist[img] = hamming_distance(img_hash[query_img], img_hash[img])\n",
    "\n",
    "    #Closest-5 images\n",
    "    img_response = sorted(img_hm_dist.items(), key=lambda item: item[1])\n",
    "    \n",
    "    for j in range(K):\n",
    "        if text_response[j][0].split('/')[0]==query_img.split('/')[0]:\n",
    "            text_cnt+=0.01\n",
    "\n",
    "        if img_response[j][0].split('/')[0]==query_img.split('/')[0]:\n",
    "            img_cnt+=0.01 \n",
    "            \n",
    "print(\"Average Precision@%d for Image-Query-Text is %0.2f\"%(K, text_cnt/len(img_hash.keys())))\n",
    "print(\"Average Precision@%d for Image-Query-Image is %0.2f\"%(K, img_cnt/len(img_hash.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@100 for Text-Query-Image is 0.62\n",
      "Average Precision@100 for Text-Query-Text is 0.60\n"
     ]
    }
   ],
   "source": [
    "#Image-Query-Text\n",
    "img_cnt, text_cnt = 0., 0.\n",
    "\n",
    "for query_text in text_hash.keys():\n",
    "    \n",
    "    img_hm_dist = {}\n",
    "    text_hm_dist = {}\n",
    "\n",
    "    for text in text_hash.keys():\n",
    "        text_hm_dist[text] = hamming_distance(text_hash[query_text], text_hash[text])\n",
    "\n",
    "    #Closest-5 texts\n",
    "    text_response = sorted(text_hm_dist.items(), key=lambda item: item[1])\n",
    "\n",
    "    for img in img_hash.keys():\n",
    "        img_hm_dist[img] = hamming_distance(text_hash[query_text], img_hash[img])\n",
    "\n",
    "    #Closest-5 images\n",
    "    img_response = sorted(img_hm_dist.items(), key=lambda item: item[1])\n",
    "    \n",
    "    for j in range(K):\n",
    "        if text_response[j][0].split('/')[0]==query_text.split('/')[0]:\n",
    "            text_cnt+=0.01\n",
    "\n",
    "        if img_response[j][0].split('/')[0]==query_text.split('/')[0]:\n",
    "            img_cnt+=0.01 \n",
    "            \n",
    "print(\"Average Precision@%d for Text-Query-Image is %0.2f\"%(K, img_cnt/len(text_hash.keys())))\n",
    "print(\"Average Precision@%d for Text-Query-Text is %0.2f\"%(K, text_cnt/len(text_hash.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
